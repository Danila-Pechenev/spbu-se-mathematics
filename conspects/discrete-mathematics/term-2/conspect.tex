\input{preamble.tex}

\begin{document}
	\Header

	\BeginConspect

	\Section{Кодирование информации}{}{Илья Дудников}

	\Subsection{Задача об оптимальном префиксном коде}

	Пусть $\Lambda$ -- произвольное конечное множество (алфавит), $a \in \Lambda$ -- символы. 
	Пусть $\forall a \in \Lambda \ \exists l(a) \in \N, \exists c(a) = \{0, 1\}^{l(a)}$ -- кодовая последовательность $a$, где $l(a)$ -- длина. 

	Очевидно, условие $\forall a, b \in \Lambda \to (a \neq b \SO c(a) \neq c(b))$ не является достаточным для однозначного распознавания символов.

	\begin{Def}
		Код называется префиксным, если $\forall a, b \in \Lambda \ c(a) = \omega \SO \not\exists m \in \N_0 : c(b) = \omega \gamma$, где $\gamma \in \{0, 1\}^m$  
	\end{Def}

	Пусть $\forall a \in \Lambda$ соответствует вероятность $p(a)$ появления этого символа в сообщении. $\sum_{a \in \Lambda} p(a) = 1$ и считаем $\forall a \in \Lambda \ p(a) > 0$. \\
	Введем дискретную случайную величину $l : \forall a \in \Lambda \ Pr\{l = l(a)\} = p(a)$ -- длина кодовой последовательности символа в сообщении.

	\begin{Def}
		Оптимальным называется префиксный код, минимизирующий математическое ожидание $l : \mathbb{E} l = \sum_{a \in \Lambda} l(a)p(a)$ 
	\end{Def}

	Чем чаще встречается символ, тем короче должна быть кодовая последовательность.

	Почему вообще ОПК существует? Известно, что $\mathbb{E} l \geqslant 1$ (в каждой кодовой последовательности должен быть хотя бы один символ).
	Всегда можно сделать префиксный код, в котором все символы имеют одинаковые длины кодовых последовательностей и эти последовательности различны
	($\forall a \in \Lambda \ l(a) = \lceil\log_2 (|\Lambda|)\rceil$), т.е. префиксный код существует и матожидание длины кодовой последовательности ограничено.

	\begin{Lm}
		Если в некотором коде $C$ существует $x \in \Lambda : c(x) = \omega \alpha$, где $\alpha \in \{0, 1\}$ и при этом $\not\exists y \in \Lambda, y \neq x : c(y) = \omega \gamma$, где $\gamma \in \{0, 1\}^k$ (то есть, если $\omega$ не является началом никакой другой кодовой последовательности, кроме $c(x)$ ),
		то код $C' : c'(x) = \omega, \forall y \in \Lambda, y \neq x \ c' = c(y)$ будет префиксным (по построению и условию леммы) и 
		$\mathbb{E}l' = \mathbb{E}l - p(x)l(x) + p(x)(l(x) - 1) = \mathbb{E} l - p(x) < \mathbb{E} l$. \\ 
		Тогда код $C$ точно не мог быть оптимальным.
	\end{Lm}

	\begin{Lm}[Лемма о кратчайшем префиксе]
		Если в префиксном коде $C \ \exists a, b \in \Lambda, a \neq b : p(a) < p(b), l(a) < l(b)$, то такой код не оптимален.
	\end{Lm}

	\begin{proof}
		Проверим, что для кода $C'$, в котором $c'(a) = c(b), c'(b) = c(a)$ и $\forall x \in \Lambda : x \neq a, x \neq b \ c'(x) = c(x)$ верно $\mathbb{E}l - \mathbb{E}l' > 0$.
		\[\mathbb{E} l - \mathbb{E} l' = p(a) l(a) + p(b) l(b) - p(a) l(b) - p(b) l(a) = (p(a) - p(b))(l(a) - l(b)) > 0\] 
	\end{proof}

	\begin{Lm}[Лемма о соседстве самых редких символов]
		Пусть $a, b \in \Lambda, a \neq b$ -- символы с намиеньшими вероятностями ( $\forall x \in \Lambda \ p(x) \geqslant p(b) \geqslant p(a)$ ). 
		Тогда $\exists $ ОПК $: c(a) = \omega 0, c(b) = \omega 1$, где $\exists k \in \N_0 : \omega \in \{0, 1\}^k$ и это самые длинные кодовые последовательности.
	\end{Lm}

	\begin{proof}
		Пусть $C'$ -- ОПК. По лемме о кратчайшем префиксе $a$ и $b$ имеют самые длинные кодовые последовательности в $C' : \forall x \in \Lambda, x \neq a, x \neq b \ l'(a) \geqslant l'(b) \geqslant l'(x)$
		
		Если $c(a) = \omega \gamma, \omega \in \{0, 1\}^{l'(b)}, \gamma \in \{0, 1\}^{l'(a) - l'(b)}$ и $\omega$ не является началом никакой кодовой последовательности
		(т.к. остальные кодовые последовательности не длиннее $\omega$ и $\not\exists $ символа с кодовой последовательностью $\omega$ в силу префиксности $C'$ ) $\SO$ 
		можно сократить кодовую последовательность $a$, создав более оптимальный код (?!).

		$\SO$ из оптимальности $C'$ следует $l(a) = l(b)$. Пусть $c'(b) = \omega 1$, тогда, если 
		$\exists x \in \Lambda : c'(x) = \omega 0$, то построим ОПК $C : c(a) = c'(x), c(x) = c'(a), \forall z \in \Lambda, z \neq a, z \neq x \ c(z) = c'(z)$. \\
		Если $\not\exists x \in \Lambda : c'(x) = \omega0$, то построим ОПК $C : c(a) = \omega0, \forall z \in \Lambda, z \neq a \ c(z) = c'(z)$.
	\end{proof}

	\begin{Lm}[Лемма об ОПК для расширенного алфавита]
		Пусть $a, b \in \Lambda, a \neq b$ -- символы с намиеньшими вероятностями.
		$\Lambda' = \Lambda \setminus \{a, b\} \cup \{\underbrace{ab}\}$, где $\underbrace{ab} \notin \Lambda$, $p(\underbrace{ab}) = p(a) + p(b)$.
		Пусть $C'$ -- ОПК для $\Lambda', c'(\underbrace{ab}) = \omega$. Тогда для $\Lambda$ код $C : c(a) = \omega 0, c(b) = \omega 1, \forall x \in \Lambda, x \neq a, x \neq b \ c(x) = c'(x)$ будет ОПК.
	\end{Lm}

	\begin{proof}
		$l(a) p(a) + l(b)p(b) = (l'(\underbrace{ab}) + 1)(p(a) + p(b)) = l'(\underbrace{ab})p(\underbrace{ab}) + p(\underbrace{ab})$. Тогда $\mathbb{E} l = \mathbb{E} l' + p(\underbrace{ab})$. \\
		Пусть $\overline{C}$ -- ОПК для $\Lambda$ и $\mathbb{E} \overline{l} < \mathbb{E} l$. По лемме о соседстве: $\overline{c}(a) = \gamma 0, \overline{c}(b) = \gamma 1$.
		Построим $\overline{C}'$ для $\Lambda' : \overline{c}'(\underbrace{ab}) = \gamma$ и $\forall x \in \Lambda, x \neq a, x \neq b \ \overline{c}'(x) = \overline{c}(x)$. 

		$\overline{C}'$ -- префиксный? По Лемме о кратчайшем префиксе $\not\exists $ символа с кодовой последовательностью длины $> \overline{l}(a)$.
		Никакой символ не мог иметь кодовую последовательность $\gamma$, т.к. $\overline{C}$ префиксный. Единственные две последовательности длины $\overline{l}(a)$, начинающиеся на $\gamma$ -- это коды $a$ и $b$.
		Но их нет в $\Lambda'$. При этом $\mathbb{E} \overline{l} = \mathbb{E} \overline{l}' = p(\underbrace{ab})$.
		По предположению $\mathbb{E}l' + p(\underbrace{ab}) = \mathbb{E}l > \mathbb{E} \overline{l} = \mathbb{E} \overline{l}' + p(\underbrace{ab})$ (?!) оптимальности $C'$ 
		$\SO \mathbb{E} \overline{l} \geqslant \mathbb{E}l$, но т.к. $\overline{C}$ -- ОПК $\SO \mathbb{E} \overline{l} = \mathbb{E} l$ и $C$ -- ОПК.
	\end{proof}

	Задача: нужно построить ОПК на алфавите $\Lambda, |\Lambda| = M$. По лемме об ОПК для расширенного алфавита задачу построения ОПК можно свести к такой же задаче, но с исходным алфавитом с числом букв на единицу меньше,
	и с набором вероятностей, получющимся из первоначального сложением двух наименьших вероятностей. \\
	Уменьшаем пока не получится алфавит из двух букв. ОПК для алфавита из 2-х букв -- $\{0, 1\}$.

	Строже: $\Lambda_0 := \Lambda$. $\forall k \in 0...(M - 3)$ берем $a_k, b_k \in \Lambda_k : \forall x \in \Lambda_k, x \neq a_k, x \neq b_k \ p(a_k) \leqslant p(b_k) \leqslant p(x)$ 
	и построим $\Lambda_{k + 1} = \Lambda_{k} \setminus \{a_k, b_k\} \cup \{\underbrace{a_kb_k}\}...$
	
	Для $\Lambda_{M - 2} = \{a_{M - 2}, b_{M - 2}\}$ оптимальным будет код $C_{M - 2} : c_{M - 2} (a_{M - 2}) = 0, c_{M - 2}(b_{M - 2}) = 1$,
	т.к. для него $\mathbb{E} l_{M - 2} = 1$. \\
	Теперь для $k \in 1 ... (M - 2)$ есть ОПК $C_k$ для $\Lambda_k$. По лемме об ОПК для расширенного алфавита строится ОПК $C_{k - 1}$ для $\Lambda_{k - 1}$ такой, что 
	$c_{k - 1} (a_{k - 1}) = c_k(a_{k - 1}b_{k - 1})0, c_{k - 1}(b_{k - 1}) = c_k (a_{k - 1}b_{k - 1})1, \forall x \in \Lambda_k, x \neq \underbrace{a_{k - 1}b_{k - 1}} \ c_{k - 1}(x) = c_k(x)$. \\
	Выполняем, пока не получится $C_0$ -- ОПК для $\Lambda_0 = \Lambda$.

	\begin{Example}
		$\Lambda_0 = \{a, b, c, d, e, f, g\}, p(a) = 0.13, p(b) = 0.08, p(c) = 0.25, p(d) = 0.18, p(e) = 0.03, p(f) = 0.12, p(g) = 0.21$. \\
		$a_0, = e, b_0 = b, \Lambda_1 = \{a, \underbrace{e, b}, c, d, f, g\}, p(a) = 0.13, p(\underbrace{eb}) = 0.11, p(c) = 0.25, p(d) = 0.18, p(f) = 0.12, p(g) = 0.21$. \\
		$a_1 = \underbrace{eb}, b_1 = f, \Lambda_2 = \{a, \underbrace{ebf}, c, d, g\}, p(a) = 0.13, p(\underbrace{ebf}) = 0.23, p(c) = 0.25, p(d) = 0.18, p(g) = 0.21$. \\ 
		$a_2 = a, b_2 = d, \Lambda_3 = \{\underbrace{ad}, \underbrace{ebf}, c, g\}, p(\underbrace{ad}) = 0.31, p(\underbrace{ebf}) = 0.23, p(c) = 0.25, p(g) = 0.21$. \\
		$a_3 = g, b_3 = \underbrace{ebf}, \Lambda_4 = \{\underbrace{ad}, \underbrace{gebf}, c\}, p(\underbrace{ad}) = 0.31, p(\underbrace{gebf}) = 0.44, p(c) = 0.25$.
		$a_4 = c, b_4 = \underbrace{ad}, \Lambda_5 \{\underbrace{cad}, \underbrace{gebf}\}, p(\underbrace{cad}) = 0.56, p(\underbrace{gebf}) = 0.44$.
		Тогда $c_5(\underbrace{gebf}) = 0, c(\underbrace{cad}) = 1$.
		
		Теперь раскрываем алфавит обратно:
		
		$c_4 (\underbrace{gebf}) = 0, c_4(c) = 10, c_4(\underbrace{ad}) = 11$. \\
		$c_3(g) = 00, c_3(\underbrace{ebf}) = 01, c_3(c) = 10, c_3(\underbrace{ad}) = 11$. \\
		$c_1(g) = 00, c_1(\underbrace{eb}) = 010, c_1(f) = 011, c_1(c) = 10, c_1(a) = 110, c_1(d) = 111$. \\
		$c_0(g) = 00, c_0(e) = 0100, c_0(b) = 0101, c_0(f) = 011, c_0(c) = 10, c_0(a) = 110, c_0(d) = 111$.
		
	\end{Example}

	\Subsection{Неравенство Крафта}

	Пусть задан набор длин $l_1, ..., l_m$, не все обязательно различны. Может ли такой набор оказаться набором длин некоторого префиксного кода.

	\begin{Thm}
		Для того, чтобы набор длин $l_1, ..., l_m$ мог быть набором длин кодовых последовательностей некоторого ПК для алфавита из $m$ символов необходимо и достаточно, чтобы $\sum_{i = 1}^m 2^{-l_i} \leqslant 1$. 
	\end{Thm}

	\def\AuthorName{Ксения Кузьмина} 

	\begin{proof}
		$"\Rightarrow". \text{ Пусть } \exists$ префиксный код для алфавита с кодовыми последовательностями с длинами $l_1, ..., l_m$. множество кодовых последовательностей -- набор всех путей на двоичном дереве от корня к листьям.\\
		Корень -- нулевой уровень. Далее последовательно увеличиваем номер по мере удаления от корня.\\
		Каждой вершине $v$ на уровне $t$ сопоставим число $a(v) = 2^{-t}$\\
		Пусть вершина $v$ на уровне $t$ -- не лист. Т.е. на уровне $t+1$ есть $\geqslant 1$ вершина, получившаяся из $v$. Обозначим её $N(v)$. Тогда $\displaystyle a(v) \geqslant \sum_{u \in N(v)} a(u)$\\ 
		Просуммируем неравенства для всех не листов: \[\sum_{v \text{ не лист }} a(v) \geqslant \sum_{u \text{ не корень }} a(u)\]\\ 
		$\displaystyle \Rightarrow 2^0 \geqslant \sum_{u \text{ листья }} a(u)$. Необходимость доказана.

		$"\Leftarrow"$. Считаем, что выполнено неравенство и пусть $l_1 \leqslant l_2 \leqslant ... \leqslant l_m$\\ 
		$n_j -$ число листьев на уровне $j: n_j = |\{i: l_i = j, i \in 1:m\}|$\\ \\
		$\displaystyle \sum_{i \in 1:m} 2^{-l_i} \geqslant 1 \Rightarrow \sum_{j \in 1:l_m} 2^{-j}n_j \leqslant$. Тогда $\forall j \in 1:l_m: n_j \leqslant 2^j - (2^{j-1}n_1 + ... + 2n_{j-1})$\\ 
		Пусть $m \neq 1$. Выделим на первом уровне вершин $n_1 \leqslant 2$, на втором уровне останется $2(2-n_1)$. Известно, что $n_2 \leqslant 2^2 - 2n_1 \Rightarrow$ осталось не меньше, чем требуется для второго уровня.\\ 
		$(j-1)-$уровень: было свободно $2^{j-1} - (2^{j-2}n_1 + ... + 2n_{j-2})$ и $n_{j-1}$ не больше этой величины. Выделим $n_{j-1}$ узлов, останется $2^{j-1} - (2^{j-2}n_1 + ... + 2n_{j-2}) - n_{j-1}$. Значит на j-м уровне будет $2 \cdot (...) = 2^{j} - (2^{j-1}n_1 + ... + 2n_{j-1})$
	\end{proof}

	\Subsection{Напоминалка}

	Пусть S -- конечное множество. $|S| = n$.\\ 
	Пусть задана функция $f: S \to [0,1], \forall \omega \in S \exists ! f(\omega) \in [0,1]$\\ 
	$\displaystyle \sum_{\omega \in S} f(\omega) = 1$. Определим $\forall A \subseteq S$ величину $\displaystyle Pr(A) = \sum_{\omega \in A} f(\omega)$\\ 
	Функция f в общем-то и не нужна. Достаточно иметь Pr. 

	\begin{Def} 
		(S, Pr) называется вероятностным пространством.\\ 
		S -- пространство элементарных событий.\\
		$\omega \in S$ -- элементарное событие (исход). $A \subseteq S$ -- событие. 
		Pr(A) -- вероятность A.\\
		$A, B \subseteq S, Pr (A \cap B) = 0$ -- несовместные события.
	\end{Def} 
	
	\textbf{Свойства вероятности:} 
	\begin{itemize}
		\item $Pr(A \cup B) = Pr(A) + Pr(B) - Pr (A \cap B)$
		\item $Pr(A) + Pr(S \backslash A) = 1$
		\item $Pr(A \cup B) \leqslant Pr(A) + Pr(B)$
		\item $Pr(A) = Pr(A \backslash B) + Pr(A \cap B)$
	\end{itemize}
	
	\textbf{Неравенство Йенсена:}

	\begin{Def} 
		Функция f называется выпуклой на $X \in R$, если $\forall x_1, x_2 \in X$ и $\forall \alpha \in [0,1]$ выполняется неравенство $f(\alpha x_1 + (1 - \alpha x_2) \leqslant \alpha f(x_1) + (1-\alpha)f(x_2)$
	\end{Def} 

	Неравенство Йенсена: пусть f выпуклая на X функция. Тогда $\displaystyle f(\sum_{i=1}^{n} \alpha_i x_i) \leqslant \sum_{i=1}^{n} \alpha_i f(x_i)$, где\\ $\displaystyle x_i \in X, \alpha_i \geqslant 0, \sum_{i=1}^{n} \alpha_i = 1$. 

	\begin{proof}
		База при n = 2 верна по определению выпуклой функции. Пусть f -- выпуклая на X функция. Тогда\\ 
		$\displaystyle f(\sum_{i=1}^{n+1} \alpha_i x_i) = f((1-\alpha_{n+1})\sum_{i=1}^{n} \frac{\alpha_i}{1-\alpha_{n+1}}x_i + \alpha_{n+1} x_{n+1}) \leqslant (1 - \alpha_{n+1}) f(\sum_{i=1}^{n} \frac{\alpha_i}{1-\alpha_{n+1}}x_i) + \alpha_{n+1} f(x_{n+1}) \leqslant \\
		\leqslant (1 - \alpha_{n+1}) \sum_{i=1}^{n} \frac{\alpha_i}{1-\alpha_{n+1}} f(x_i) + \alpha_{n+1} f(x_{n+1}) = \sum_{i=1}^{n+1} \alpha_i f(x_i)$
	\end{proof}

	\Subsection{Конечная случайная схема}
	\begin{Def} 
		Пусть $A_1, A_2, ..., A_n -$ разбиение множества исходов S вероятностного пространства (S, Pr). Конечной случайной схемой называется схема $\alpha$, сопоставляющая каждому $A_i$ вероятность $Pr(A_i)$
	\end{Def} 

	\begin{Def} 
		Энтропией КСС называется $\displaystyle H(\alpha) = - \sum_{i=1}^{n} Pr(A_i) \times \log Pr(A_i)$
	\end{Def} 

	\textbf{Свойства энтропии:}
	\begin{itemize}
		\item $H(\alpha) \geqslant 0$
		\item Энтропия характеризует неопределенность, заключенную в КСС
		\item Для любой $\alpha \subset k$ исходами справедливо $H(\alpha) \leqslant \log k$ 
	\end{itemize}
	 
	\begin{proof}
		$f(x) := -x \cdot \log x.$ На $[0,1]$ функция $f(x)$ строго вогнутая $\Rightarrow$ по неравенству Йенсена $\displaystyle \sum_{i=1}^{n} \lambda_i \cdot f(x_i) \leqslant f(\sum_{i=1}^{n} \lambda_i \cdot x_i),$ причём равенство $\Leftrightarrow x_1 = ... = x_n$.\\
		Тогда возьмём $x_i = Pr(A_i)$ и $\lambda_i = \frac{1}{k} \ \forall i \in 1...k,$ получаем $\displaystyle \sum_{i=1}^{k} \frac{1}{k} (-Pr(A_i) \times \log Pr(A_i)) \leqslant \\ \leqslant - \sum_{i=1}^{k} \frac{1}{k} Pr(A_i) \times \log(\sum_{i=1}^{k} Pr(A_i))$\\
		$\displaystyle - \frac{1}{k} \sum_{i=1}^{k} Pr(A_i) \times \log Pr(A_i) \leqslant - \frac{1}{k} \log \frac{1}{k}$\\
		$\displaystyle - \sum_{i=1}^{k} Pr(A_i) \times \log Pr(A_i) \leqslant \log k$\\
		Максимальная энтропия для КСС имеет схема с k равновероятностными исходами.\\
		$H(\alpha) = 0 \Leftrightarrow \exists !$ достоверный исход в $\alpha $
	\end{proof}

	Пусть есть КСС $\alpha$ с исходами $A_1, ..., A_k$  и КСС $\beta$  с исходами $B_1, ..., B_l$. Их пересечением $\alpha \cap \beta $ называются КСС, исходы которой -- $A_i \cap B_j, \forall i \in 1, ..., k, j \in 1, ..., l$\\
	Тогда $\displaystyle H(\alpha \cap \beta ) = - \sum_{i=1}^{k} \sum_{j=1}^{l} Pr(A_i \cap B_j) \times \log Pr(A_i \cap B_j)$\\ \\ 
	Т.к. $\displaystyle Pr(A_i \cap B_j) = Pr(A_i) \times Pr(B_j | A_i) \Rightarrow H(\alpha \cap \beta) = \\
	 = - \sum_{i=1}^{k} \sum_{j=1}^{l} Pr(A_i)Pr(B_j|A_i) \times (\log Pr(A_i) + \log Pr(B_j | A_i)) = \\ 
	 = - \sum_{i=1}^{k} \sum_{j=1}^{l} Pr(A_i)Pr(B_j | A_i) \times \log Pr(A_i) - \sum_{i=1}^{k} \sum_{j=1}^{l} Pr(A_i) Pr(B_j | A_i) \times \log Pr(B_j | A_i) = \\
	 = - \sum_{i=1}^{k} Pr(A_i) \cdot \log Pr(A_i) \cdot \sum_{j=1}^{l} Pr(B_j | A_i) + \sum_{i=1}^{k} Pr(A_i) \cdot (- \sum_{j=1}^{l} Pr(B_j | A_i) \cdot \log Pr(B_j | A_i)) =\\
	 = - \sum_{i=1}^{k} Pr(A_i) \cdot \log Pr(A_i) + ... = H(\alpha) + ...$

	 \begin{Def} 
		Величину $H(\beta | A_i) := - \sum_{j=1}^{l} Pr(A_i) Pr(B_j | A_i) \cdot \log Pr(B_j | A_i)$ называют условной энтропией $\beta$ при условии $A_i$ 
	\end{Def} 

	\begin{Def} 
		Величину $H_{\alpha} (\beta) := \sum_{i=1}^{k} Pr(A_i) \cdot H(\beta | A_i)$ называют средней условной энтропией $\beta$ при условии $\alpha$.
	\end{Def} 

	Таким образом, $H(\alpha \cap \beta) = H(\alpha) + H_{\alpha}(\beta)$\\
	Докажем, что $0 \leqslant H_{\alpha}(\beta) \leqslant H(\beta)$\\
	Неотрицательность следует из неотрицательности энтропий. \\
	fix $j, \ f(x) = - x \cdot \log x, \lambda_i = Pr(A_i), x_i = Pr(B_j | A_i) \ \  \forall i \in 1...k$

	Неравенство Йенсена: $\displaystyle \sum_{i=1}^{k} Pr(A_i) \cdot (- Pr(B_j | A_i) \cdot \log Pr(A_i)) \leqslant \\
	\leqslant (- \sum_{i=1}^{k} Pr(A_i) \cdot Pr(B_j | A_i)) \cdot \log \sum_{i=1}^{k} Pr(A_i) \cdot Pr(B_j | A_i)$\\
	ПЧ $\displaystyle = (- \sum_{i=1}^{k} Pr(A_i) \cdot Pr(B_j | A_i)) \cdot \log \sum_{i=1}^{k} Pr(A_i) \cdot Pr(B_j | A_i) = \\
	= -(\sum_{i=1}^{k} Pr(B_j \cap A_i)) \cdot \log \sum_{i=1}^{k} Pr(B_j \cap A_i) = -Pr(B_j) \cdot \log Pr(B_j)$ 

	Просуммируем по j:
	$\displaystyle \sum_{j=1}^{j} \sum_{i=1}^{k} Pr(A_i) \cdot (- Pr(B_j | A_i) \cdot \log Pr(A_i)) \leqslant \sum_{j=1}^{l} (- Pr(B_j) \cdot \log Pr(B_j))\\
	\sum_{i=1}^{k} Pr(A_i) \cdot \sum_{j=1}^{l} (- Pr(B_j | A_i) \cdot \log Pr(A_i)) \leqslant - \sum_{j=1}^{l} Pr(B_j) \cdot \log Pr(B_j)\\
	\sum_{i=1}^{k} Pr(A_i) \cdot H(\beta | A_i) \leqslant H(\beta) \Rightarrow H_{\alpha} (\beta) \leqslant H(\beta)\\
	H_{\alpha} (\beta) = H(\beta) \Leftrightarrow$ все $Pr(B_j | A_i)$ равны между собой.\\
	Формула полной вероятности: $\displaystyle \forall j \in 1...l Pr(B_j) = \sum_{i=1}^{k} Pr(B_j | A_i) \cdot Pr(A_i)\\
	\forall j \in 1...l Pr(B_j) = Pr(B_j | A_1) \cdot \sum_{i=1}^{k} Pr(A_i) = Pr(B_j | A_1).$\\
	То есть $\forall i \in 1...k, j \in 1...l \ \  Pr(B_j) = Pr(B_j | A_i)$

	\begin{Def} 
		События A и B -- взаимно независимы $\Leftrightarrow Pr(A \cap B) = Pr(A) \cdot Pr(B) \Leftrightarrow Pr(A) \cdot Pr(B|A) = Pr(A) \cdot Pr(B) \Leftrightarrow Pr(B|A) = Pr(B)$
	\end{Def} 

	\begin{Def} 
	КСС $\alpha$ и $\beta$ называются независимыми, когда все исходы $\alpha$ независимы со всеми исходами $\beta$. В таком случае $H_{\alpha} (\beta)$ максимальна и равна $H(\beta)$
	\end{Def} 
	
	\Subsection{Количество информации}

	\begin{Def} 
		Величина $I(\alpha, \beta) = H(\beta) - H_{\alpha} (\beta)$ называется количеством информации.
	\end{Def} 

	\textbf{Свойства:}
	\begin{itemize}
		\item $I(\alpha, \beta) \geqslant 0$
		\item $I(\alpha, \beta) = H(\beta) \Leftrightarrow H_{\alpha} (\beta) = 0$
		\item $I(\alpha, \beta) = I(\beta, \alpha)$
		\item $I(\alpha, \beta) = 0 \Leftrightarrow \alpha \text{ и } \beta$ независимы.
	\end{itemize}
	
	\textbf{Пример:}\\
	Загадано натуральное число $x \in 1...N\\
	\beta -$ опыт, состоящий в нахождении x, $\beta_m$ -- опыт, показывающий, делится ли $x$ на $m$, $m \in 1...N$.\\ У $\beta$ есть N исходов, у $\beta_m$ -- два исхода.\\
	$\displaystyle H_{\beta_m}(\beta) = Pr (x \vdots m) \cdot H(\beta | x \vdots m) + Pr (x \nmid m) \cdot H (\beta | x \nmid m)\\
	q := \Big| \frac{N}{m} \Big|$ -- количество чисел от 1 до N, делящихся на m. Тогда $\displaystyle Pr(x \vdots m) = \frac{q}{N}, Pr(x \nmid m) = \frac{N-q}{N}.\\
	H(\beta|x \vdots m) = - \sum_{i \vdots m, i \in 1...m} \frac{1}{q} \cdot \log \frac{1}{q} = - \frac{q}{q} \cdot \log \frac{1}{q} = \log q$.\\
	Аналогично $\displaystyle H(\beta | x \nmid m) = \log (N-q) \Rightarrow H_{\beta_m}(\beta) = \frac{q}{N} \cdot \log q + \frac{N-q}{N} \cdot \log (N-q)$\\
	$\displaystyle I()\beta_m, \beta) = \log N - \frac{q}{N} \cdot \log q - \frac{N-q}{N} \cdot \log (N-q) = \\
	= \frac{q}{N} \cdot \log N - \frac{q}{N} \log q - \frac{N-q}{N} \cdot \log N - \frac{N-q}{N} \cdot \log (N-q) =\\
	= - \frac{q}{N} \cdot \log \frac{q}{N} - \frac{N-q}{N} \cdot \log \frac{N-q}{N} \leqslant \log 2$\\
	Равенство  достигается при $q = N - q = \frac{N}{2}$.\\ \\ 

	\textbf{Данетки:}\\ 
	Загадано число от 1 до $N$.\\
	Опыт $\beta$ -- угадать число.\\
	Опыт $\alpha$ -- задать любой общий (да/нет) вопрос и получить ответ.\\
	$H(\beta) = \log N$ (Числа загаданы с равной вероятностью)\\
	$H(\alpha) \leqslant \log 2$ (Поскольку есть всего 2 варианта ответа)\\
	$H(\alpha_1 \alpha_2 ... \alpha_k) \leqslant \log 2^k = k \log 2$ (k вопросов, 2 варианта ответа)\\
	Чтобы угадать число потребуется $k \geqslant \frac{\log N}{\log 2} = \log_2 N$ вопросов.\\
	Есть ли алгоритм, который умеет угадывать загаданное число за $O(\log N)$.\\ \\

	\textbf{Избыточное кодирование:}\\
	Есть сообщение $u \in \{0, 1\}^k$, которое нужно передать.\\
	Можем передавать сообщение $x(u) \in \{0, 1\}^n, n \geqslant k,$ содержащую некоторую избыточную информацию (канал связи шумит и может допускать ошибки), но не более $d$ ошибок на сообщение.\\
	$\beta$ заключается в нахождении всех $d$ ошибок. Сколько у $\beta$ исходов? Для каждого количества ошибок $j$ от 0 до $d$ есть $\binom{n}{j}$ вариантов их расположения, то есть всего исходов у $\beta$ ровно $\displaystyle \sum_{j=0}^{d} \binom{n}{j}$ \\
	Следовательно, $\displaystyle H(\beta) = \log \sum_{j=0}^{d} \binom{n}{j}$\\
	$\alpha$ -- дополнительное сообщение размера $n - k$. Их $2^{n-k}$ и $\Rightarrow H(\alpha) = \log 2^{n-k} = (n-k) \log 2$\\
	Чтобы гарантированно найти все ошибки нужно $H(\alpha) \geqslant H(\beta)$
	\[(n-k) \log 2 \geqslant \log \sum_{j=0}^{d} \binom{n}{j} \Rightarrow n-k \geqslant \log_2 \sum_{j=0}^{d} \binom{n}{j} \Rightarrow k \leqslant n - \log_2 \sum_{j=0}^{d} \binom{n}{j}\]
	Таким образом, если канал связи допускает не более $d$ ошибок, то для передачи сообщения размером k понадобится не менее $k + \log_2 \sum_{j=0}^{d} \binom{n}{j}$.\\
	Или, поскольку количество ошибок обычно зависит от размера переданного сообщения, если передаётся n бит и из них не более $d$ могут быть ошибочными, то в переданном сообщении можно закодировать сообщение длиной не более $\displaystyle n - \log_2 \sum_{j=0}^{d} \binom{n}{j}$. 
	
	\textbf{Код Хэмминга:}\\
	Предыдущая задача при $d = 1$. Известно, что $\displaystyle 2^{n-k} \geqslant \sum_{j=0}^{1} \binom{n}{j} = 1+n$.\\
	$l := n - k -$ длина "избыточного сообщения". Тогда $k \leqslant 2^l - l - 1$.\\
	Чем большее сообщение, тем относительно меньше лишней информации. Как передавать дополнительную информацию?

	%Вообще здесь по-хорошему сделать бы когда-нибудь таблицы

	\textbf{Пример:}\\
	Пусть k = 12  и мы хотим передать сообщение $u = 101101011100$. Зарезервируем в сообщении длины 17 места с номерами $2^i (1, 2, 4, 8, 16)$, а на остальные позиции запишем сообщение:\\ $x_0 (u) = \_ \_ 1 \_ 011 \_ 0101110 \_ 0$\\
	Подберём на позицию $2^i$ такую цифру, чтобы произведение $x(u)$ и $i$-й строки матрицы было равно 0. На "неопределенных" позициях в строке с номером $i$ стоят $0 (2^j = 10 \dots 0)$.\\
	На позиции $2^i$ в $i-$й строке стоит 1.\\
	$?*1+\_*0+1*1+\_*0+0*1+1*0+1*1+\_*0+0*1+1*0+0*1+1*0+1*1+1*0+0*1+\_*0+0*1 = \\ = ? + 1 + 1 + 1 = 1 + ? = 0 \Rightarrow ? = 1$.\\
	Получается $x_1 = 1\_1\_011\_0101110\_0$\\
	Аналогично делаем для остальных. Итого $x(u) = 11110110010111000$\\
	Как определять позицию ошибки? $y = 11110110000111000$\\
	Посчитаем $A \times y^{T} = (0, 1, 0, 1, 0)^{T}$ -- двоичная запись позиции с ошибкой.\\
	Старший бит -- справа. Почему так? %%Видимо, продолжение в следующей серии :)

	%\def\AuthorName{раскомментируйте и вставьте сюда имя следующего, кто будет над этим страдать} 

\end{document}